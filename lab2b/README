NAME: Eugene Choi
EMAIL: echoi2@g.ucla.edu
ID: 905368197

Files Included:

	lab2_list.c:
	Implementation of code that initializes a circular sorted doubly linked list, appends to it, grabs its size, and deletes its elements while utilizing different options that are entered from the command line. Emphasises concurrency. Available options to include in command line are --threads, --iterations, --yield, --sync, --lists.

	Makefile:
	Generates files for submission using dist, cleans the directory using clean option, populates the .csv file by running tests.sh via tests option, creates .png files using the graphs options, and creates the .out file using the profile option.

	lab2b_list.csv:
	Contains the data that is generated by running the make tests command which runs the lab2_list executable using a variety of commmand line options such as ./lab2_list --threads=2 --iterations=10.

	profile.out:
	Execution profiling report that shows where time was spent in the un-partitioned spin-lock implementation.

	lab2b_1.png:
	The graph generated by the Makefile option (make graphs) that shows throughput vs. number of threads for mutex and spin-lock synchronized list operations.

	lab2b_2.png:
	The graph generated by the Makefile option (make graphs) that shows mean time per mutex wait and mean time per operation for mutex-synchronized list operations.

	lab2b_3.png:
	The graph generated by the Makefile option (make graphs) that shows successful iterations vs. threads for each synchronization method.

	lab2b_4.png:
	The graph generated by the Makefile option (make graphs) that shows throughput vs. number of threads for mutex synchronized partitioned lists.

	lab2b_5.png:
	The graph generated by the Makefile option (make graphs) that shows throughput vs. number of threads for spin-lock-synchronized partitioned lists.

	tests.sh:
	Test script used to generate data for the .csv file which the .gp file extracts data points from to create graphs

	README:
	Contains explanation of each file, the answers to the questions mentioned in the spec, and a list of the sources referenced to create the project.

	lab2_list.gp:
	Contains the code for creating the .png graphs with the desired outputs.




QUESTION 2.3.1 - CPU time in the basic list implementation
	Answer:	I believe that most of the CPU time in 1 and 2 thread list tests are spent performing the insert, lookup, delete, and list size operations for the list. The operations are the most expensive parts of the code when smaller amounts of threads are involved, such as 1 or 2, because for a single thread there is no wait time for the obtaining of the lock and for double threads there aren't multiple threads competing for the lock making the availability of the lock happen much quicker. This leaves much more time being spent on actually executing the operations. In high-thread spin-lock tests, most of the CPU time is being spent in the thread spinning while waiting to acquire the lock. In high thread mutex tests, most of the CPU time is spent on the list operations because unlike while utilizing spin lock protection where cycles are spent having a thread spin while waiting to acquire a lock, in mutex protection a thread that cannot acquire the lock is put to sleep until the lock is available to the thread (doesn't waste cycles). 


QUESTION 2.3.2 - Execution Profiling
	Answer: Lines 353 and 484 seem to be consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads. These lines in particular involve the spinning of the thread while it waits to acquire the lock from the given sublist (signified by the while(__sync_lock_test_and_set(&lock[key_location], 1)) statement). This operation becomes expensive with large amounts of threads because there are many more threads competing to acquire the lock thus leading to more threads spinning in wait leading the CPU time for lock acquisition to increase. 


QUESTION 2.3.3 - Mutex Wait Time
	Answer: The average lock-wait time rises dramatically with the number of contending threads because of competition between the multiple threads to obtain the lock and execute the critical section of code that is locked by the mutex. What ends up happening is that while one thread locks the region to execute that piece of code, the schedular might decide to perform a context switch which unblocks the multiple threads ready to execute that region of code. Those threads compete with each other with only one thread making it to execution where it will be put back to sleep if that critical section is still locked. The context switches keep on happening, but threads will always be put back to sleep until that critical section is unlocked by the thread that locked it, leading to the dramatic increase in the average lock-wait time. Completion time per operation rises less dramatically with the number of contending threads because there will always be a thread executing a part of the program while the other threads wait for the resource to become available (means that work is still being done towards completion instead of there just being a random halt in the process toward completion) unless there is a deadlock where no thread will be able to execute the critical section. It is possible for the wait time per operation to go up faster/higher than the completion time per operation because the wait time per operation is a summation of the wait time for all the threads (wait times that can overlap with one another meaning that even if you have two threads waiting the same amount of time, those two times both get added to the total wait time giving a much larger value) divided by the total number of lock operations which can result in much higher times than the completion time per operation. 

QUESTION 2.3.4 - Performance of Partitioned Lists
	Answer:	As the number of lists grows, the throughput becomes much better for both spin lock and mutex lock protection. The throughput should increase which means that performance increases until a ceiling is hit (ceiling being when wait times for threads to acquire a lock become zero because each thread will be able to go into a different sublist to obtain a lock because there will be an abundance due to the increased list size). From the curves, it is safe to assume that the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. When looking at the curves, we can see that the single list seems to increase in throughput with a lesser amount of threads for both the spin-lock and the mutex protection options while the N-way partitioned list seems to decrease in throughput with the more threads that it has for both spin-lock and mutex protection options.




Sources Used:
http://gnuplot.10905.n7.nabble.com/title-font-size-td17474.html
https://linux.die.net/man/3/pthread_mutex_init
https://stackoverflow.com/questions/25340496/why-does-modulus-operator-work-for-char-but-not-for-floating-types
https://www.tutorialspoint.com/cprogramming/c_data_types.htm
https://stackoverflow.com/questions/14320041/pthread-mutex-initializer-vs-pthread-mutex-init-mutex-param
https://kahdev.wordpress.com/2008/07/21/generating-random-characters-in-c/ learned how to make random chars for keys via this reference
https://www.tutorialspoint.com/cprogramming/c_data_types.htm The reason why you can mod char with integers in c is because char is considered an integer type in c.
https://stackoverflow.com/questions/16016920/type-casting-integer-to-void
https://www.geeksforgeeks.org/mutex-lock-for-linux-thread-synchronization/
Referenced a lot from lab 2a and from the discussion notes